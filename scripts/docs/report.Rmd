---
title: "Sarcasm/irony project"
output: html_document
date: "2025-03-18"
---

```{r setup, include=FALSE}
library(tidyverse)
library(here)
library(bayestestR)
library(bayesplot)
library(brms)
library(report)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

syrett_theme <- function() {
  theme(
    axis.text = element_text(colour = "black", family = "Arial", size = 12),
    axis.title = element_text(colour = "black", family = "Arial", size = 12))
} 

tidy_data = read.csv(here("data", "tidy_data.csv")) %>% 
  filter(trial_number > 6) # removing practice trials 
```

<details>
  <summary>**Trial probe: Approve**</summary>

**Statistical Analysis**

This section contains the analysis for the trial probe "approve". First, the total proportion of responses ("no" coded as 1 or "yes" coded as 2) was compared as a function of intonation type (sarcastic or declarative). Next, the proportions of responses were compared according to the second predictor alone, "modifier_type" (3 levels: +gradable +subjective +valence adjectives, 
+gradable +subjective –valence adjective, 
–gradable –subjective adjectives/nouns). 
Finally, both predictors are visualized and considered in tandem as they relate to the responses of "yes" or "no".

In each case, a Bayesian multilevel logistic regression was run in which the binary outcome (log-odds of choosing "yes") was analyzed as a function of the relevant predictors (detailed below) and a random intercept for subject id, trial object, and trial modifier. 
Each model was estimated using MCMC
sampling with 4 chains of 2000 iterations and a warmup of 1000
Hamiltonian Monte-Carlo sampling was carried out distributed between 6 processing cores.

**Intonation only**

```{r, include=FALSE}
approve_df = tidy_data %>% 
  filter(probe_type == "approve") 

pcts_app = approve_df %>% group_by(intonation, response) %>% summarise(n = round(n()/834, digits = 2)*100)
```

```{r}
approve_df %>% 
  mutate(response = case_when(
    response == 1 ~ "No",
    response == 2 ~ "Yes",
  )) %>% 
  ggplot(aes(intonation, fill = as.factor(response), group = as.factor(response))) + 
  geom_bar(position = "fill", color = "black") +
  theme_minimal() + syrett_theme() + scale_fill_discrete(name = "Response") + ggtitle("Overall proportion of responses to declarative and sarcastic items")

```

The first plot shows the overall proportion of responses to declarative and sarcastic items (before taking other conditions into account).
Overall, choosing "yes" was slightly more frequent for the declarative items (`r pcts_app$n[2]` percent) compared to `r pcts_app$n[4]` percent in sarcastic responses.

A logistic regression model was fit in which response was predicted as a function of intonation (including the random effects mentioned above). 
The model, described below, found substantial evidence that the overall difference between responses to sarcastic and declarative items was not due to chance (probability of direction approaching 1).
The table below shows the predicted log-odds for a selection of "no" for the declarative condition (represented by the intercept in the first row) and the effect (change in log-odds) of sarcastic (second row).
The negative effect suggests that a selection of "no" is less likely for sarcastic items than declarative ones ("yes" is more likely).

```{r}
approve_mod_int = brms::brm(response ~ intonation + (1 | subject.ID) + 
                       (1 | trial_object) + (1 | trial_modifier),
                     family = "bernoulli",
                     data = approve_df,
                     file = here("data", "models", "approve_mod_intonation_on;y.rds"))

describe_posterior(
  approve_mod_int,
  effects = "fixed",
  component = "all",
  test = c("p_direction", "p_significance"),
  centrality = "all"
) %>%
  as.data.frame() %>% 
  mutate(across(where(is.numeric), round, 3)) %>% 
  knitr::kable(row.names = FALSE)
```

**Modifier type only**

```{r}
approve_df %>% 
    mutate(response = case_when(
    response == 1 ~ "No",
    response == 2 ~ "Yes",
  )) %>% 
  ggplot(aes(modifier_type, fill = as.factor(response), group = as.factor(response))) + 
  geom_bar(position = "fill", color = "black") +
  theme_minimal() + syrett_theme() + scale_fill_discrete(name = "Response") + ggtitle("Overall proportion of responses to the modifier types")
```

```{r, include=FALSE}
pcts_app_mod = approve_df %>% group_by(modifier_type, response) %>% summarise(n = round(n()/556, digits = 2)*100)
```

This plot shows the overall proportion of responses to the three modifier types .
Overall, a response of "yes" was most frequent for the `grad+subj+pos` items (`r pcts_app_mod$n[4]` percent), followed by `–grad–subj` with `r pcts_app_mod$n[6]` percent of responses of "yes".
Finally, `grad+subj+neg` showed very few responses of "yes" (only `r pcts_app_mod$n[2]` percent).

A logistic regression model was fit in which response was predicted as a function of modifier type (including the random effects mentioned above). 

The model, described below, found substantial evidence that, relative to the baseline `–grad–subj`, there was compelling evidence that both of the other modifier types were distinctly responded to by participants. 
The table below shows the predicted log-odds for a selection of "no" for the `grad–subj` condition (represented by the intercept in the first row).
The model predicts that a choice of "no" for `modifier_typegradPsubjPneg` (second row) relative to `grad–subj` is lower overall.
The opposite was found for the modifier type `b_modifier_typegradPsubjPpos` relative to `grad-subj`, in which the log-odds of a choice of "no" was higher.

```{r}
approve_mod_int_mt = brms::brm(response ~ modifier_type + (1 | subject.ID) + 
                       (1 | trial_object) + (1 | trial_modifier),
                     family = "bernoulli",
                     data = approve_df,
                     file = here("data", "models", "approve_mod_mod_type_only.rds"))

describe_posterior(
  approve_mod_int_mt,
  effects = "fixed",
  component = "all",
  test = c("p_direction", "p_significance"),
  centrality = "all"
) %>%
  as.data.frame() %>% 
  mutate(across(where(is.numeric), round, 3)) %>% 
  knitr::kable(row.names = FALSE)
```

**Intonation and Modifier type**

```{r}
approve_df %>% 
    mutate(response = case_when(
    response == 1 ~ "No",
    response == 2 ~ "Yes",
  )) %>% 
  ggplot(aes(modifier_type, fill = as.factor(response), group = as.factor(response))) + 
  geom_bar(position = "fill", color = "black") + facet_wrap(~intonation) +
  theme_minimal() + syrett_theme() + scale_fill_discrete(name = "Response") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

```{r, include=FALSE}
approve_all = approve_df %>% group_by(intonation, response, modifier_type) %>% summarise(n = round(n()/278, digits = 2))

```

Please see a descriptive summary of each of the six cases below:

When the intonation was `r approve_all$intonation[1]` and the modifier type was `r approve_all$modifier_type[1]`, `r approve_all$n[1]` percent of responses were "no".

When the intonation was `r approve_all$intonation[2]` and the modifier type was `r approve_all$modifier_type[2]`, `r approve_all$n[2]` percent of responses were "no".


When the intonation was `r approve_all$intonation[3]` and the modifier type was `r approve_all$modifier_type[3]`, `r approve_all$n[3]` percent of responses were "no".

When the intonation was `r approve_all$intonation[7]` and the modifier type was `r approve_all$modifier_type[7]`, `r approve_all$n[7]` percent of responses were "no".


When the intonation was `r approve_all$intonation[8]` and the modifier type was `r approve_all$modifier_type[8]`, `r approve_all$n[8]` percent of responses were "no".

When the intonation was `r approve_all$intonation[9]` and the modifier type was `r approve_all$modifier_type[9]`, `r approve_all$n[9]` percent of responses were "no".

The final model is a little more complex than the first two, since we now have an interaction. 
This model predicts to log-odds of a response of as a function of both intonation (2 levels) and modifier type (3 levels) and their interaction. 
As a result, the intercept of this model represents the log-odds of a choice of "no" when the modifier type is `grad–subj` and the intonation is `declarative`. 
The fixed effect `b_intonationsarcastic` represents the change in the predicted log-dds of choosing "no" when we change the predictor for intonation from `declarative` to `sarcastic`. It's negative, which suggests that it the probality of choosing "no" is less for sarcastic intonation relative to declarative when the modifier type is `grad–subj`. 
Rows 3 and 4 make similar adjustments, but this time change modifier type while holding the intonation constant. 
The final two terms are the interaction terms. 
These represent the change relative to the declarative modifier combinations. The positive estimate for row 5 suggests that the probability of choosing "no" when the modifier type is `grad+subj+neg` is higher for when the intonation is sarcastic than declarative. 
The negative value for row 6 suggests that the probability of choosing "no" when the modifier type is `grad+subj+pos` is lower for when the intonation is sarcastic than declarative. 
In each comparison, the row "pd" (probability of direction) tell us how sure we can be about the given comparison. In this case, all of the comparisons' directions are highly probable (over .95). 

```{r}
approve_mod = brms::brm(response ~ intonation*modifier_type + (1 | subject.ID) + 
                       (1 | trial_object) + (1 | trial_modifier),
                     family = "bernoulli",
                     data = approve_df,
                     file = here("data", "models", "approve_mod.rds"))

describe_posterior(
  approve_mod,
  effects = "fixed",
  component = "all",
  test = c("p_direction", "p_significance"),
  centrality = "all"
) %>%
  as.data.frame() %>% 
  mutate(across(where(is.numeric), round, 3)) %>% 
  knitr::kable(row.names = FALSE)
```

</details> 

<details>
  <summary>**Trial probe: Mean**</summary>
  
```{r}
mean_df = tidy_data %>% 
  filter(probe_type == "mean") 

mean_mod = brms::brm(response ~ intonation*modifier_type + (1 | subject.ID) + 
                            (1 | trial_object) + (1 | trial_modifier),
                          family = "bernoulli",
                          data = mean_df,
                     file = here("data", "models", "mean_mod.rds"))

```


```{r, include=FALSE}
pcts_mean = mean_df %>% group_by(intonation, response) %>% summarise(n = round(n()/834, digits = 2)*100)
```

```{r}
mean_df %>% 
    mutate(response = case_when(
    response == 1 ~ "No",
    response == 2 ~ "Yes",
  )) %>% 
  ggplot(aes(intonation, fill = as.factor(response), group = as.factor(response))) + 
  geom_bar(position = "fill", color = "black") +
  theme_minimal() + syrett_theme() + scale_fill_discrete(name = "Response") + ggtitle("Overall proportion of responses to declarative and sarcastic items")

```

The first plot shows the overall proportion of responses to declarative and sarcastic items (before taking other conditions into account).
Again (like "approve" itmes), coding of "yes" was more frequent for the declarative items (`r pcts_mean$n[2]` percent) compared to `r pcts_mean$n[4]` percent in sarcastic responses.


**Note: This is exactly the same as the approve section**

A logistic regression model was fit in which response was predicted as a function of intonation (including the random effects mentioned above). 
The model, described below, found substantial evidence that the overall difference between responses to sarcastic and declarative items was not due to chance (probability of direction approaching 1).
The table below shows the predicted log-odds for a selection of "no" for the declarative condition (represented by the intercept in the first row) and the effect (change in log-odds) of sarcastic (second row).
The negative effect suggests that a selection of "no" is less likely for sarcastic items than declarative ones.

```{r}
mean_mod_int = brms::brm(response ~ intonation + (1 | subject.ID) + 
                       (1 | trial_object) + (1 | trial_modifier),
                     family = "bernoulli",
                     data = mean_df,
                     file = here("data", "models", "mean_mod_intonation_on;y.rds"))

describe_posterior(
  mean_mod_int,
  effects = "fixed",
  component = "all",
  test = c("p_direction", "p_significance"),
  centrality = "all"
) %>%
  as.data.frame() %>% 
  mutate(across(where(is.numeric), round, 3)) %>% 
  knitr::kable(row.names = FALSE)
```

**Modifier type only**

```{r}
mean_df %>% 
    mutate(response = case_when(
    response == 1 ~ "No",
    response == 2 ~ "Yes",
  )) %>% 
  ggplot(aes(modifier_type, fill = as.factor(response), group = as.factor(response))) + 
  geom_bar(position = "fill", color = "black") +
  theme_minimal() + syrett_theme() + scale_fill_discrete(name = "Response") + ggtitle("Overall proportion of responses to the modifier types")
```

```{r, include=FALSE}
pcts_mean_mod = mean_df %>% group_by(modifier_type, response) %>% summarise(n = round(n()/556, digits = 2)*100)
```

This plot shows the overall proportion of responses to the three modifier types .
Overall, coding of "yes" was most frequent for the `grad+subj+neg` items (`r pcts_app_mod$n[2]` percent), followed by `–grad–subj` with `r pcts_app_mod$n[6]` percent of responses of "yes".
Finally, `grad+subj+pos` showed very few responses of "yes" (only `r pcts_app_mod$n[4]` percent).

A logistic regression model was fit in which response was predicted as a function of modifier type (including the random effects mentioned above). 

The model, described below, found substantial evidence that, relative to the baseline `–grad–subj`, there was compelling evidence that both of the other modifier types were distinctly responded to by participants. 
The table below shows the predicted log-odds for a selection of "no" for the `grad–subj` condition (represented by the intercept in the first row).
The model predicts that a choice of "no" for `modifier_typegradPsubjPneg` (second row) relative to `grad–subj` is higher overall.
The opposite was found for the modifier type `b_modifier_typegradPsubjPpos` relative to `grad-subj`, in which the log-odds of a choice of "no" was lower

```{r}
mean_mod_int_mt = brms::brm(response ~ modifier_type + (1 | subject.ID) + 
                       (1 | trial_object) + (1 | trial_modifier),
                     family = "bernoulli",
                     data = mean_df,
                     file = here("data", "models", "mean_mod_mod_type_only.rds"))

describe_posterior(
  mean_mod_int_mt,
  effects = "fixed",
  component = "all",
  test = c("p_direction", "p_significance"),
  centrality = "all"
) %>%
  as.data.frame() %>% 
  mutate(across(where(is.numeric), round, 3)) %>% 
  knitr::kable(row.names = FALSE)
```

**Intonation and Modifier type**

```{r}
mean_df %>% 
    mutate(response = case_when(
    response == 1 ~ "No",
    response == 2 ~ "Yes",
  )) %>% 
  ggplot(aes(modifier_type, fill = as.factor(response), group = as.factor(response))) + 
  geom_bar(position = "fill", color = "black") + facet_wrap(~intonation) +
  theme_minimal() + syrett_theme() + scale_fill_discrete(name = "Response") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

```{r, include=FALSE}
mean_all = mean_df %>% group_by(intonation, response, modifier_type) %>% summarise(n = round(n()/278, digits = 2))
```

Please see a descriptive summary of each of the six cases below:

When the intonation was `r mean_all$intonation[1]` and the modifier type was `r mean_all$modifier_type[1]`, `r mean_all$n[1]` percent of responses were "no".

When the intonation was `r mean_all$intonation[2]` and the modifier type was `r mean_all$modifier_type[2]`, `r mean_all$n[2]` percent of responses were "no".

When the intonation was `r mean_all$intonation[3]` and the modifier type was `r mean_all$modifier_type[3]`, `r mean_all$n[3]` percent of responses were "no".


When the intonation was `r mean_all$intonation[7]` and the modifier type was `r mean_all$modifier_type[7]`, `r mean_all$n[7]` percent of responses were "no".

When the intonation was `r mean_all$intonation[8]` and the modifier type was `r mean_all$modifier_type[8]`, `r mean_all$n[8]` percent of responses were "no".

When the intonation was `r mean_all$intonation[9]` and the modifier type was `r mean_all$modifier_type[9]`, `r mean_all$n[9]` percent of responses were "no".


Like the previous model with an interaction (for "approve"), this model predicts to log-odds of a response of as a function of both intonation (2 levels) and modifier type (3 levels) and their interaction. 
As a result, the intercept of this model represents the log-odds of a choice of "no" when the modifier type is `grad–subj` and the intonation is `declarative`. 
The fixed effect `b_intonationsarcastic` represents the change in the predicted log-dds of choosing "no" when we change the predictor for intonation from `declarative` to `sarcastic`. It's negative, which suggests that it the probability of choosing "no" is less for sarcastic intonation relative to declarative when the modifier type is `grad–subj`. 
Rows 3 and 4 make similar adjustments, but this time change modifier type while holding the intonation constant. 
The final two terms are the interaction terms. 
These represent the change relative to the declarative modifier combinations. The negative estimate for row 5 suggests that the probability of choosing "no" when the modifier type is `grad+subj+neg` is lower for when the intonation is sarcastic than declarative. 
The negative value for row 6 suggests that the probability of choosing "no" when the modifier type is `grad+subj+pos` is lower for when the intonation is sarcastic than declarative. 
This model does not provide compelling evidence for each comparison (see the "pd" column - those lower than .95 are uncertain).

```{r}
mead_mod = brms::brm(response ~ intonation*modifier_type + (1 | subject.ID) + 
                       (1 | trial_object) + (1 | trial_modifier),
                     family = "bernoulli",
                     data = mean_df,
                     file = here("data", "models", "mean_mod.rds"))

describe_posterior(
  mead_mod,
  effects = "fixed",
  component = "all",
  test = c("p_direction", "p_significance"),
  centrality = "all"
) %>%
  as.data.frame() %>% 
  mutate(across(where(is.numeric), round, 3)) %>% 
  knitr::kable(row.names = FALSE)
```


</details> 

<details>
  <summary>**Trial probe: Nice**</summary>
```{r, include=FALSE}
nice_df = tidy_data %>% 
  filter(probe_type == "nice") 

overall_pcts = tidy_data %>% 
  filter(probe_type == "nice") %>% 
  group_by(response) %>% 
  summarize(pct_rating = n()/1668)


intonation_pcts = tidy_data %>% 
  filter(probe_type == "nice") %>% 
  group_by(intonation, response) %>% 
  summarize(pct_rating = n()/834)

intonation_mt_pcts = tidy_data %>% 
  filter(probe_type == "nice") %>% 
  group_by(intonation, modifier_type, response) %>% 
  summarize(pct_rating = n()/278)


```

The plot below shows the distribution of ratings overall (not taking into account intonation or modifier type). 
The most frequent ratings were 2 (chosen 33% of the time), 3 (26%) and 4 (18.55).
5 was the least frequent rating (8.5%) followed by 1 (14%).

```{r}
tidy_data %>% 
  filter(probe_type == "nice") %>% 
  ggplot(aes(x = response, fill = as.factor(response), group = as.factor(response))) + 
  geom_bar(position = "dodge", color = "black") +
  theme_minimal() + syrett_theme() + theme(legend.position="none") + ggtitle("Overall distribution of ratings")
```

This plot shows the ratings according to intonation.
The declarative items (left) follow a normal distribution, in which 3 was chosen most often (31.2%), followed by 2 and 4 (26.4% and 21.6% respectively), and finally by the outer most choices, 1 (11.9%) and 5 (8.9%).
The sarcastic items show a left skew. 
The most frequent rating for these items was 2 (39.8%) of ratings, followed by 3 (20.7%), and 1 (16.1%). 4 and 5 were chosen the least (15.3% and 8% respectively).

```{r}
tidy_data %>% 
  filter(probe_type == "nice") %>% 
  ggplot(aes(x = response, fill = as.factor(response), group = as.factor(response))) +
  geom_bar(position = "dodge", color = "black") +
  theme_minimal() + syrett_theme() + facet_wrap(~intonation) +
  theme(legend.position="none") + ggtitle("Distribution of ratings according to intonation")

nice_mod_int = brms::brm(response ~ intonation + (1 | subject.ID) + 
                          (1 | trial_object) + (1 | trial_modifier),
                        family = "cumulative",
                        data = nice_df, 
                     file = here("data", "models", "nice_mod_int.rds"))

```

The ordinal model is reported below (a Bayesian ordinal logistic regression).
In this model, the so-called "intercepts" are actually cut off points.
Imagine we are drawing a rectangle on a piece of paper and we want to divide it into 5 parts, where each part represents one of the possible responses (1-5). We divide the rectangle by drawing a vertical line and creating a new section. 
We want the area of the part of the rectangle to be proportional to the quantity of the response (e.g. if 38% of the choices are 2, we would want 38% of the rectangle to be the "2" block").
To divide a single rectangle into five parts, we would draw four lines.
The location of these lines on the horizontal axis (which determines the relative area of each parts) is what the model predicts.
Basically, `Intercept[1]` predicts the first cut (and thus, the probability of a repsonse of 1) in the baseline condition (in this case, declarative intonation). 
`Intercept[2]` is the second cut (the point between 2 and 3), and so on.
The predictor `intonationsarcastic` assesses whether there is a difference in ratings based on whether the statement was declarative or a sarcastic. 
The model suggests that there was - and specifically that the difference was -.75 log-odds [95% HDI -.94 - -.55]. 
The probability of direction was (approaching) 1. 

```{r}
describe_posterior(
  nice_mod_int,
  effects = "fixed",
  component = "all",
  test = c("p_direction", "p_significance"),
  centrality = "all"
) %>%
  as.data.frame() %>% 
  mutate(across(where(is.numeric), round, 3)) %>% 
  knitr::kable(row.names = FALSE)
```


The final plot breaks down ratings into modifier type and intonation. A table below lists each specific percentage. 

```{r}
tidy_data %>% 
  filter(probe_type == "nice") %>% 
  ggplot(aes(x = response, fill = as.factor(response), group = as.factor(response))) +
  geom_bar(position = "dodge", color = "black") +
  theme_minimal() + syrett_theme() + facet_grid(modifier_type~intonation) +
  theme(legend.position="none") + ggtitle("Distribution of ratings according to intonation and modifier type")
```

```{r}
intonation_mt_pcts %>% 
  pivot_wider(names_from = response, values_from = pct_rating) %>% 
  knitr::kable(format = "pandoc")
```

```{r}
nice_mod = brms::brm(response ~ intonation*modifier_type + (1 | subject.ID) + 
                          (1 | trial_object) + (1 | trial_modifier),
                        family = "cumulative",
                        data = nice_df, 
                     file = here("data", "models", "nice_mod.rds"))
```

The complete model is detailed below. As usual, this one is more complicated due to the interaction. 
In this model, the intercepts represent the break points for the baseline condition (declarative intonation and –grad–subj modifier type).
The fixed effect for sarcastic now compares declarative intonation and –grad–subj modifier type with sarcastic intonation and –grad–subj modifier type (a negative, compelling effect).
The effect `b_modifier_typegradPsubjPneg` compares `declarative-grad+subj+neg` to `declarative 	–grad–subj` (the baseline). 
The effect is a again negative and compelling (with `declarative-grad+subj+neg` being less).
The fixed effect `b_modifier_typegradPsubjPpos` found the opposite: `declarative-grad+subj+pos` had higher ratings than the baseline `declarative 	–grad–subj`.	
The effect `b_intonationsarcastic:modifier_typegradPsubjPneg` compares `declarative-grad+subj+pos` to `sarcastic-grad+subj+pos` and found higher ratings for the latter.
Finally, `b_intonationsarcastic:modifier_typegradPsubjPpos` provides evidence that `sarcastic-grad+subj+pos` was rated lower than `declarative-grad+subj+pos`, though this difference was not compelling (pd = .81).
The other comparisons were all compelling with their probabilities of direction approaching 1. 


```{r}
describe_posterior(
  nice_mod,
  effects = "fixed",
  component = "all",
  test = c("p_direction", "p_significance"),
  centrality = "all"
) %>%
  as.data.frame() %>% 
  mutate(across(where(is.numeric), round, 3)) %>% 
  knitr::kable(row.names = FALSE)
```


</details> 